---
# Source: kray/helm-charts/vllm-llama-8b/templates/deployment.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-llama-8b
  labels:
    helm.sh/chart: vllm-1.0.0
    app.kubernetes.io/name: vllmllama-8b
    app.kubernetes.io/instance: vllmllama-8b
    app.kubernetes.io/version: "2.1.0"
    app.kubernetes.io/managed-by: Helm
spec:
  # use explicit replica counts only of HorizontalPodAutoscaler is disabled
  replicas: 2
  selector:
    matchLabels:
      app.kubernetes.io/name: vllmllama-8b
      app.kubernetes.io/instance: vllmllama-8b
  template:
    metadata:
      labels:
        app.kubernetes.io/name: vllmllama-8b
        app.kubernetes.io/instance: vllmllama-8b
    spec:
      securityContext:
        {}
      hostIPC: true
      containers:
        - name: vllm
          envFrom:
            - configMapRef:
                name: vllm-llama-8b-config
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
              add:
                - SYS_NICE
            readOnlyRootFilesystem: false
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          image: "opea/vllm-gaudi:latest"
          env:
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hugging-face-token
                  key: hugging-face-token
            - name: MODEL_ID
              value: '$(LLM_MODEL_ID)'
          command:
            - /bin/bash
            - -c
            - |
              export OMPI_MCA_btl_vader_single_copy_mechanism=$(OMPI_MCA_btl_vader_single_copy_mechanism) && \
              export PT_HPU_ENABLE_LAZY_COLLECTIVES=$(PT_HPU_ENABLE_LAZY_COLLECTIVES) && \
              export HABANA_VISIBLE_DEVICES=$(HABANA_VISIBLE_DEVICES) && \
              export runtime=$(RUNTIME) && \
              export VLLM_NO_USAGE_STATS=1 && \
              export DO_NOT_TRACK=1 && \
              export VLLM_CPU_KVCACHE_SPACE=$(VLLM_CPU_KVCACHE_SPACE) && \
              python3 -m vllm.entrypoints.openai.api_server --dtype $(DTYPE)  --model $(LLM_MODEL_ID) --port $(PORT) --tensor-parallel-size $(TENSOR_PARALLEL_SIZE) --disable-log-requests --block-size $(BLOCK_SIZE) --max-model-len $(MAX_MODEL_LEN)
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - mountPath: /data
              name: model-volume
            - mountPath: /tmp
              name: tmp
          ports:
            - name: http
              containerPort: 2080
              protocol: TCP
          livenessProbe:
            failureThreshold: 60
            initialDelaySeconds: 450
            periodSeconds: 30
            tcpSocket:
              port: http
          readinessProbe:
            initialDelaySeconds: 450
            periodSeconds: 30
            tcpSocket:
              port: http
          startupProbe:
            failureThreshold: 300
            initialDelaySeconds: 5
            periodSeconds: 5
            tcpSocket:
              port: http
          resources:
            limits:
              habana.ai/gaudi: 1

      volumes:
        - name: model-volume
          emptyDir: {}
        - name: tmp
          emptyDir: {}

