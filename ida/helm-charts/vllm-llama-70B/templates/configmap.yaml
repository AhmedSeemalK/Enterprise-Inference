---
# Source: kray/helm-charts/vllm-llama-70b/templates/configmap.yaml
# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

apiVersion: v1
kind: ConfigMap
metadata:
  name: vllm-llama-70b-config
  labels:
    helm.sh/chart: vllm-1.0.0
    app.kubernetes.io/name: vllmllama-70b
    app.kubernetes.io/instance: vllmllama-70b
    app.kubernetes.io/version: "2.1.0"
    app.kubernetes.io/managed-by: Helm
data:
  PORT: "2080"
  HF_TOKEN: "{{ .Values.hfToken }}"
  http_proxy: ""
  https_proxy: ""
  no_proxy: ""
  HABANA_LOGS: "/tmp/habana_logs"
  NUMBA_CACHE_DIR: "/tmp"
  HF_HOME: "/tmp/.cache/huggingface"
  MAX_INPUT_LENGTH: "1024"
  MAX_TOTAL_TOKENS: "2048"
  TRANSFORMERS_CACHE: "/tmp/transformers_cache"
  NUM_SHARD: "8"
  SHARDED: "true"
  OMPI_MCA_btl_vader_single_copy_mechanism: "none"
  PT_HPU_ENABLE_LAZY_COLLECTIVES: "true"
  HABANA_VISIBLE_DEVICES: "all"
  VLLM_CPU_KVCACHE_SPACE: "40"
  TENSOR_PARALLEL_SIZE: "8"
  BLOCK_SIZE: "128"
  DTYPE: "bfloat16"
  MAX_MODEL_LEN: "5120"
  RUNTIME: "habana"
  LLM_MODEL_ID: "meta-llama/Meta-Llama-3.1-70B-Instruct"
